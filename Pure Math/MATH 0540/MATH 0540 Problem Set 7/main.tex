\documentclass[12pt,reqno]{article}

%%%%%%%%%%%%%%%%%%%% PACKAGES %%%%%%%%%%%%%%%%%%%%

\usepackage[utf8]{inputenc}
\usepackage[all]{xy}
\usepackage[T1]{fontenc}
\usepackage[usenames, dvipsnames]{color}
\usepackage{setspace}
\usepackage{dsfont}
\usepackage{amssymb}
\usepackage{amsthm,bbm}
\usepackage{amscd}
\usepackage{amsfonts}
\usepackage{stmaryrd}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{xspace}
\usepackage{extarrows}
\usepackage{color}
\usepackage [english]{babel}
\usepackage [autostyle, english = american]{csquotes}
\usepackage{euler}

%%%%%%%%%%%%%%%%%%%% INITIALIZATION %%%%%%%%%%%%%%%%%%%%

\MakeOuterQuote{"}
\graphicspath{ {./} }
\onehalfspacing

%%%%%%%%%%%%%%%%%%%% ENVIRONMENTS %%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{problem}{Problem}

%%%%%%%%%%%%%%%%%%%% TITLE-PAGE %%%%%%%%%%%%%%%%%%%%

\title{MATH 0540 Problem Set 7}
\author{Collaborated with EsmÃ©}
\date{October 2022}

%%%%%%%%%%%%%%%%%%%% DOCUMENT %%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%% PROBLEM 1 %%%%%%%%%%%%%%%%%%%%

\begin{problem}
Let $T: \mathbb{R}^2 \rightarrow \mathbb{R}^3$ be the linear map given by
$$
T(x,y) = (2x-3y,5x+7y,11x).
$$
With respect to the bases $\{(1,1),(1,-2)\}$ on $\mathbb{R}^2$ and $\{(-1,0,1),(5,1,0),(0,3,1)\}$ on $\mathbb{R}^3$, compute $M(T)$.
\end{problem}

\begin{proof}
    A linear map is defined by its behavior on a basis of its domain, therefore let's input $(1, 1)$ and $(1, -2)$ into $T$:
    \begin{align*}
        &(2(1) - 3(1), 5(1) + 7(1), 11(1)) &= &\quad(-1, 12, 11)\\
        &(2(1) - 3(-2), 5(1) + 7(-2), 11(1)) &= &\quad(8, -9, 11)
    \end{align*}
    Thus, $M(T)$ with respect to the standard basis of of $\mathbb{R}^3$ is:
    \begin{align*}
        \begin{bmatrix}
            -1 & 8 \\
            12 & -9 \\
            11 & 11
        \end{bmatrix}
    \end{align*}
    \\ The desired basis vectors of $\mathbb{R}^3$ can be expressed as the following matrix with respect to the standard basis of $\mathbb{R}^3$:
    \begin{align*}
        \begin{bmatrix}
            -1 & 5 & 0 \\
            0 & 1 & 3 \\
            1 & 0 & 1
        \end{bmatrix}
    \end{align*}
    \\ Next, we will transform our $M(T)$ with respect to the standard basis of $\mathbb{R}^3$ to be in terms of these new basis vectors.
    \begin{align*}
        \left[\begin{array}{ccc|cc}
             -1 & 5 & 0 & -1 & 8\\
            0 & 1 & 3 & 12 & -9\\
            1 & 0 & 1 & 11 & 11
        \end{array}\right]
        &\xrightarrow[]{r_2\ \to\ r_2 - 3r_3}
        \left[\begin{array}{ccc|cc}
             -1 & 5 & 0 & -1 & 8\\
            -3 & 1 & 0 & -21 & -42\\
            1 & 0 & 1 & 11 & 11
        \end{array}\right]
        \\ &\xrightarrow[]{r_1\ \to\ r_1 - 5r_2}
        \left[\begin{array}{ccc|cc}
             14 & 0 & 0 & 104 & 218\\
            -3 & 1 & 0 & -21 & -42\\
            1 & 0 & 1 & 11 & 11
        \end{array}\right]
        \\ &\xrightarrow[]{r_1\ \to\ r_1 \div 14}
        \left[\begin{array}{ccc|cc}
             1 & 0 & 0 & \frac{52}{7} & \frac{109}{7}\\
            -3 & 1 & 0 & -21 & -42\\
            1 & 0 & 1 & 11 & 11
        \end{array}\right]
        \\ &\xrightarrow[]{r_2\ \to\ r_2 + 3r_1}
        \left[\begin{array}{ccc|cc}
             1 & 0 & 0 & \frac{52}{7} & \frac{109}{7}\\
            0 & 1 & 0 & \frac{9}{7} & \frac{33}{7}\\
            1 & 0 & 1 & 11 & 11
        \end{array}\right]
        \\ &\xrightarrow[]{r_3\ \to\ r_3 - r_1}
        \left[\begin{array}{ccc|cc}
             1 & 0 & 0 & \frac{52}{7} & \frac{109}{7}\\
            0 & 1 & 0 & \frac{9}{7} & \frac{33}{7}\\
            0 & 0 & 1 & \frac{25}{7} & \frac{-32}{7}
        \end{array}\right]
    \end{align*}
    Therefore, $M(T)$ with respect to the bases $\{(1,1),(1,-2)\}$ on $\mathbb{R}^2$ and $\{(-1,0,1),(5,1,0),(0,3,1)\}$ on $\mathbb{R}^3$ is:
    \begin{align*}
        \begin{bmatrix}
            \frac{52}{7} & \frac{109}{7}\\
            \frac{9}{7} & \frac{33}{7}\\
            \frac{25}{7} & \frac{-32}{7}
        \end{bmatrix}
    \end{align*}
\end{proof}
\newpage

%%%%%%%%%%%%%%%%%%%% PROBLEM 2 %%%%%%%%%%%%%%%%%%%%

\begin{problem}
    A square matrix $A \in F^{n \times n}$ is called \emph{upper triangular} if $A_{j,k} = 0$ whenever $j > k$. Let $A$ and $C$ be upper triangular matrices. Prove that $AC$ is upper triangular.
\end{problem}

\begin{proof}
    Let $A$ and $C$ be upper triangular matrices of the same dimension. Thus, each element of $AC$ can be defined as:
    \begin{align*}
        (AC)_{j,k} &= \sum_{r = 1}^{n}A_{j,r}C_{r,k} & \text{(Axler, 3.41)}
    \end{align*}
    To show that $AC$ is upper triangular, we will prove that all $(AC)_{j,k} = 0$ when $j > k$. First, it is evident that for any upper triangular matrix $M(T) \in \mathbb{F}^{n\times n$, $M(T)_{j,r} = 0$ when $r < j$ and $M(T)_{r,k} = 0$ when $r > k$. Therefore, for all $j > k$:
    \begin{align*}
        (AC)_{j,k} &= \sum_{r = 1}^{n} A_{j,r}C_{r,k} & \text{(Axler, 3.41)}\\
        &= \sum_{r = 1}^{j - 1}A_{j,r}C_{r,k} + \sum_{r = j}^{n}A_{j,r}C_{r,k} & \text{(split sum)}\\
        &= \sum_{r = 1}^{j - 1}0 \cdot C_{r,k} + \sum_{r = j}^{n}A_{j,r} \cdot 0 & \text{(definition of upper triangular)}\\
        &= \sum_{r = 1}^{j - 1}0 + \sum_{r = j}^{n}0 & \text{(multiplication)}\\
        &= 0 & \text{(0 is additive identity)}
    \end{align*}
    Therefore, we have that $AC$ is upper triangular.
\end{proof}
\newpage

%%%%%%%%%%%%%%%%%%%% PROBLEM 3 %%%%%%%%%%%%%%%%%%%%

\begin{problem}
    Prove or disprove: If $A$ in $\mathbb{R}^{2 \times 2}$, then  $A^2 = 0$ if and only if $A=0$.
\end{problem}

\begin{proof}
    Let $A \in \mathbb{R}^{2 \times 2}$ be the non-zero matrix
    \begin{bmatrix}
        0 & 0\\
        1 & 0
    \end{bmatrix}.
    \begin{align*}
        A^2 &=
        \begin{bmatrix}
            0 & 0\\
            1 & 0
        \end{bmatrix}
        \begin{bmatrix}
            0 & 0\\
            1 & 0
        \end{bmatrix}
        & \text{(definition of square)}\\
        &=
        \begin{bmatrix}
            0\cdot 0 + 0\cdot 1 & 0\cdot 0 + 0\cdot 0\\
            1\cdot 0 + 0\cdot 1 & 1\cdot 0 + 0\cdot 0
        \end{bmatrix}
        & \text{(matrix multiplication)}\\
        &=
        \begin{bmatrix}
            0+0 & 0+0 \\
            0+0 & 0+0
        \end{bmatrix}
        & \text{(multiplication)}\\
        &=
        \begin{bmatrix}
            0 & 0 \\
            0 & 0
        \end{bmatrix}
        & \text{(0 is additive identity)}\\
        &= 0 & \text{(definition of 0 matrix)}
    \end{align*}
    Therefore, since we have shown that $A^2 = 0$ for some non-zero $A$ in $\mathbb{R}^{2 \times 2}$, we have disproven that if $A$ in $\mathbb{R}^{2 \times 2}$, then  $A^2 = 0$ if and only if $A=0$.
\end{proof}
\newpage

%%%%%%%%%%%%%%%%%%%% PROBLEM 4 %%%%%%%%%%%%%%%%%%%%

\begin{problem} Given a vector $v\in V$, and a basis $v_1, \ldots, v_n$ of $V$, the \emph{matrix of $v$} with respect to this basis is the $n \times 1$ matrix 
$$
M(v) = \begin{bmatrix}
c_1 \\ 
\vdots \\
c_n
\end{bmatrix},
$$
where $c_1, \ldots, c_n$ are the scalars such that $v = c_1 v_1 + 
\cdots + c_n v_n$. With this definition, prove that if $T \in L(V,W)$, with $w_1, \ldots, w_m$ a basis of $W$, then 
$$
M(T(v)) = M(T)M(v).
$$
\end{problem}

\begin{proof}
    $T(v)$ will be some vector $w \in W$, which can be represented as a linear combination of the basis vectors of $w$. The matrix, $M(T(v))$ will therefore contain the coefficients, $a_1,\ldots,a_m \in \mathbb{F}$, of this linear combination:
    \begin{align*}
        M(T(v)) = M(w) = M(a_1w_1+\ldots+a_mw_m) =
        \begin{bmatrix}
            a_1 \\ \vdots \\ a_m
        \end{bmatrix}
    \end{align*}
    We can now rewrite $M(T(v)) = M(T)M(v)$ as:
    \begin{align*}
        \begin{bmatrix}
            a_1 \\ \vdots \\ a_m
        \end{bmatrix}
        =
        \begin{bmatrix}
            M(T)_{1,1} & \cdots & M(T)_{1,n}\\
            \vdots & & \vdots \\
            M(T)_{m, 1} & \cdots & M(T)_{m, n}
        \end{bmatrix}
        \begin{bmatrix}
            c_1 \\ \vdots \\ c_n
        \end{bmatrix}
    \end{align*}
    By the definition of matrix multiplication, we have that:
    \begin{align*}
        a_i = M(T)_{i,1}c_1+\ldots+M(T)_{i,n}c_n = \sum_{j=1}^{n}M(T)_{i,j}c_j
    \end{align*}
    To prove that $M(T(v)) = M(T)M(v)$, we will show that this relationship is true for all $a_i \in a_1,\ldots,a_m$.
    \begin{align*}
        a_1w_1+\ldots+a_mw_m &= T(v) & \text{(definition of T)}\\
        &= T(v_1c_1+\ldots+v_nc_n) & \text{(linear combination)}\\
        &= T(v_1c_1)+\ldots+T(v_nc_n) & \text{(additivity)}\\
        &= c_1T(v_1)+\ldots+c_nT(v_n) & \text{(homogeneity)}\\
        &= c_1(\sum_{j=1}^{m}M(T)_{j,1}w_j)+\ldots+c_n(\sum_{j=1}^{m}M(T)_{j,n}w_j) & \text{(Axler, 3.32)}\\
        &= \sum_{j=1}^{m}c_1M(T)_{j,1}w_j+\ldots+\sum_{j=1}^{m}c_nM(T)_{j,n}w_j & \text{(distributive property)}\\
        &= \sum_{j=1}^{n}c_jM(T)_{1,j}w_1+\ldots+\sum_{j=1}^{n}c_jM(T)_{m,j}w_m & \text{(commutativity)}\\
        &= (\sum_{j=1}^{n}c_jM(T)_{1,j})w_1+\ldots+(\sum_{j=1}^{n}c_jM(T)_{m,j})w_m & \text{(distributive property)}\\
        &= (\sum_{j=1}^{n}M(T)_{1,j}c_j)w_1+\ldots+(\sum_{j=1}^{n}M(T)_{m,j}c_j)w_m & \text{(commutativity)}\\
        a_i &= \sum_{j=1}^{n}M(T)_{i,j}c_j & \text{(equate coefficients)}
    \end{align*}
    Therefore, since each row of $M(T(v))$ follows the desired relationship, we have proven that $M(T(v)) = M(T)M(v)$.
\end{proof}
\newpage

%%%%%%%%%%%%%%%%%%%% PROBLEM 5 %%%%%%%%%%%%%%%%%%%%

\begin{problem}
    What is the multiplicative identity in $R^{n \times n}$? I.e., write an $n\times n $ matrix $I$ such that $MI=IM$ for any $M \in \mathbb{R}^{n\times n}$, and prove that your matrix $I$ has this property.
\end{problem}

\begin{proof}
    Define $I \in \mathbb{R}^{n\times n}$ such that $I_{j,k} = 1$ if $j = k$, otherwise $I_{j,k} = 0$:
    \begin{align*}
        \begin{bmatrix}
            1 & 0 & \cdots & 0\\
            0 & 1 & & \vdots \\
            \vdots &  & \ddots & 0\\
            0 & \cdots & 0 & 1
        \end{bmatrix}
    \end{align*}
    Now, we will show that $MI=IM$ for any $M \in \mathbb{R}^{n\times n}$.
    \begin{align*}
        MI &=
        \begin{bmatrix}
            M(T)_{1,1} & \cdots & M(T)_{1,n}\\
            \vdots & & \vdots\\
            M(T)_{n,1} & \cdots & M(T)_{n,n}
        \end{bmatrix}
        \begin{bmatrix}
            1 & 0 & \cdots & 0\\
            0 & 1 & & \vdots \\
            \vdots &  & \ddots & 0\\
            0 & \cdots & 0 & 1
        \end{bmatrix}
        \text{(definition of $M$ and $I$)}\\
        &=
        \begin{bmatrix}
            \sum_{j=1}^{n}M(T)_{1,j}I_{j,1} & \cdots & \sum_{j=1}^{n}M(T)_{1,j}I_{j,n}\\
            \vdots & & \vdots \\
            \sum_{j=1}^{n}M(T)_{n,j}I_{j,1} & \cdots & \sum_{j=1}^{n}M(T)_{n,j}I_{j,n}
        \end{bmatrix}
        \text{(matrix multiplication)}\\
        &=
        \begin{bmatrix}
            \text{\tiny{$M(T)_{1,1}\cdot 1+M(T)_{1,2}\cdot 0+\ldots+M(T)_{1,n}\cdot 0$}} & \cdots & \text{\tiny{$M(T)_{1,1}\cdot 0+\ldots+M(T)_{1,n-1}\cdot 0+M(T)_{1,n}\cdot 1$}}\\
            \vdots & & \vdots \\
            \text{\tiny{$M(T)_{n,1}\cdot 1+M(T)_{n,2}\cdot 0+\ldots+M(T)_{n,n}\cdot 0$}} & \cdots & \text{\tiny{$M(T)_{n,1}\cdot 0+\ldots+M(T)_{n,n-1}\cdot 0+M(T)_{n,n}\cdot 1$}}
        \end{bmatrix}
        \text{(definition of $I$)}\\
        &=
        \begin{bmatrix}
            M(T)_{1,1} & \cdots & M(T)_{1,n}\\
            \vdots & & \vdots\\
            M(T)_{n,1} & \cdots & M(T)_{n,n}
        \end{bmatrix}
        \text{(multiplication)}
    \end{align*}
    \begin{align*}
        IM &=
        \begin{bmatrix}
            1 & 0 & \cdots & 0\\
            0 & 1 & & \vdots \\
            \vdots &  & \ddots & 0\\
            0 & \cdots & 0 & 1
        \end{bmatrix}
        \begin{bmatrix}
            M(T)_{1,1} & \cdots & M(T)_{1,n}\\
            \vdots & & \vdots\\
            M(T)_{n,1} & \cdots & M(T)_{n,n}
        \end{bmatrix}
        \text{(definition of $I$ and $M$)}\\
        &=
        \begin{bmatrix}
            \sum_{j=1}^{n}I_{1,j}M(T)_{j,1} & \cdots & \sum_{j=1}^{n}I_{1,j}M(T)_{j,n}\\
            \vdots & & \vdots \\
            \sum_{j=1}^{n}I_{n,j}M(T)_{j,1} & \cdots & \sum_{j=1}^{n}I_{n,j}M(T)_{j,n}\\
        \end{bmatrix}
        \text{(matrix multiplication)}\\
        &=
        \begin{bmatrix}
            \text{\tiny{$1\cdot M(T)_{1,1}+0\cdot M(T)_{2,1}+\ldots+0\cdot M(T)_{n,1}$}} & \cdots & \text{\tiny{$1\cdot M(T)_{1,n}+0\cdot M(T)_{2,n}+\ldots+0\cdot M(T)_{n,n}$}}\\
            \vdots & & \vdots \\
            \text{\tiny{$0\cdot M(T)_{1,1}+\ldots+0\cdot M(T)_{n-1,1}+1\cdot M(T)_{n,1}$}} & \cdots & \text{\tiny{$0\cdot M(T)_{1,n}+\ldots+0\cdot M(T)_{n-1,n}+1\cdot M(T)_{n,n}$}}
        \end{bmatrix}
        \text{(definition of $I$)}\\
        &=
        \begin{bmatrix}
            M(T)_{1,1} & \cdots & M(T)_{1,n}\\
            \vdots & & \vdots\\
            M(T)_{n,1} & \cdots & M(T)_{n,n}
        \end{bmatrix}
        \text{(multiplication)}
    \end{align*}
    Therefore, we have proven that $MI=IM$.
\end{proof}

\end{document}